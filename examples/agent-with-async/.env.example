# =============================================================================
# Async AI-Driven Agent - Environment Configuration
# =============================================================================
# Copy this file to .env and configure your settings:
#   cp .env.example .env
#
# This agent uses CHAIN CLIENT for production-grade resilience:
#   - Automatic failover: OpenAI -> Anthropic -> Groq
#   - If OpenAI fails (rate limit, outage), automatically tries Anthropic
#   - If Anthropic fails, falls back to Groq
#   - You only need ONE provider configured, but more = better resilience
# =============================================================================

# =============================================================================
# AI Provider API Keys (Chain Client: OpenAI -> Anthropic -> Groq)
# =============================================================================
# The agent tries providers in order until one succeeds.
# Configure multiple providers for high availability.
#
# Priority order:
#   1. OpenAI (primary) - Best quality, most features
#   2. Anthropic (fallback) - Excellent reasoning, different strengths
#   3. Groq (final fallback) - Ultra-fast, generous free tier

# OpenAI - Primary Provider (RECOMMENDED)
# Get your key: https://platform.openai.com/api-keys
# Pricing: https://openai.com/pricing
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic - First Fallback (Claude models)
# Get your key: https://console.anthropic.com/
# Pricing: https://www.anthropic.com/pricing
#ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Groq - Final Fallback (Ultra-fast inference)
# Get your key: https://console.groq.com/keys
# Free tier: 14,000 tokens/minute - great for development!
#GROQ_API_KEY=gsk-your-groq-key-here

# Google Gemini - Alternative Provider
# Get your key: https://aistudio.google.com/apikey
#GEMINI_API_KEY=your-gemini-key-here

# DeepSeek - Advanced Reasoning (OpenAI-compatible)
# Get your key: https://platform.deepseek.com/
#DEEPSEEK_API_KEY=sk-your-deepseek-key-here

# =============================================================================
# Model Alias Overrides (Optional)
# =============================================================================
# Override which model is used for each "alias" without changing code.
# Pattern: GOMIND_{PROVIDER}_MODEL_{ALIAS}=actual-model-name
#
# Aliases:
#   - "default": General use, balanced cost/quality
#   - "smart": Best reasoning quality
#   - "fast": Speed and cost optimized
#
# Example: Use cheaper models in development
GOMIND_OPENAI_MODEL_DEFAULT=gpt-4o-mini
#GOMIND_OPENAI_MODEL_SMART=gpt-4o
#GOMIND_ANTHROPIC_MODEL_DEFAULT=claude-3-haiku-20240307
#GOMIND_GROQ_MODEL_DEFAULT=llama-3.1-8b-instant

# =============================================================================
# Service Configuration
# =============================================================================

# Redis URL for task queue and service discovery (required)
REDIS_URL=redis://localhost:6379

# HTTP server port
PORT=8098

# Number of background workers (embedded mode only)
WORKER_COUNT=3

# Kubernetes namespace for service discovery (required for K8s deployment)
#NAMESPACE=gomind-examples

# Development mode - enables verbose logging
DEV_MODE=false

# =============================================================================
# Deployment Mode (Production)
# =============================================================================
# GOMIND_MODE controls how the agent runs:
#   - "" (unset): Embedded mode - API + Workers in same process (local dev)
#   - "api": API server only - handles HTTP requests, enqueues tasks
#   - "worker": Worker only - processes tasks from Redis queue
#
# For production, deploy separate api and worker pods using the same image
#GOMIND_MODE=api
#GOMIND_MODE=worker

# =============================================================================
# Orchestration Configuration (Optional)
# =============================================================================
# These control how the agent orchestrates tool calls and workflows.

# Orchestrator mode: autonomous or workflow
#GOMIND_ORCHESTRATOR_MODE=autonomous

# Number of tools to consider for each step
#GOMIND_CAPABILITY_TOP_K=20

# Similarity threshold for tool matching (0.0-1.0)
#GOMIND_CAPABILITY_THRESHOLD=0.7

# =============================================================================
# Semantic Retry Configuration (Layer 4: Contextual Re-Resolution)
# =============================================================================
# When a tool call fails with validation error (400/422), the semantic retry
# system uses LLM to re-compute parameters based on execution context.
#
# Example: "sell 100 TSLA shares and convert to KRW"
#   - get_stock_quote(TSLA) returns {price: 468.285}
#   - convert_currency(amount: 0) fails with "amount must be > 0"
#   - Semantic retry: LLM computes 100 * 468.285 = 46828.5 and retries

#GOMIND_SEMANTIC_RETRY_ENABLED=true
#GOMIND_SEMANTIC_RETRY_MAX_ATTEMPTS=2

# =============================================================================
# LLM Debug Payload Store (Production Debugging)
# =============================================================================
# Captures complete LLM prompts and responses for debugging orchestration issues.
# Unlike Jaeger spans which truncate large payloads, this stores the full content.
#
# Recording sites: plan_generation, correction, synthesis, synthesis_streaming,
#                  micro_resolution, semantic_retry
#
# See: orchestration/notes/LLM_DEBUG_PAYLOAD_DESIGN.md

# Enable LLM debug capture (default: false)
GOMIND_LLM_DEBUG_ENABLED=true

# TTL for successful records (default: 24h)
#GOMIND_LLM_DEBUG_TTL=24h

# TTL for error records - longer for debugging (default: 168h / 7 days)
#GOMIND_LLM_DEBUG_ERROR_TTL=168h

# Redis database index for debug storage (default: 7)
#GOMIND_LLM_DEBUG_REDIS_DB=7

# =============================================================================
# Telemetry Configuration (Optional)
# =============================================================================
# APP_ENV selects the telemetry profile:
#   - development: Console output, high verbosity, all traces
#   - staging: OTLP export, medium verbosity, sampled traces
#   - production: OTLP export, low verbosity, sampled traces

APP_ENV=development

# OpenTelemetry endpoint for distributed tracing
# In Kubernetes: http://otel-collector.gomind-examples.svc.cluster.local:4318
# Local with port-forward: http://localhost:4318
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# =============================================================================
# Logging Configuration (Optional)
# =============================================================================
# GOMIND_LOG_LEVEL: debug, info, warn, error
#GOMIND_LOG_LEVEL=info

# Alternative debug flag
#GOMIND_DEBUG=true
