# =============================================================================
# Research Agent with Telemetry - Environment Configuration
# =============================================================================
# Copy this file to .env and configure your settings:
#   cp .env.example .env
#
# This agent demonstrates FULL OBSERVABILITY with:
#   - Distributed tracing (Jaeger/Grafana Tempo)
#   - Metrics export (Prometheus)
#   - Structured logging with trace correlation
#   - AI request tracing with token usage
#
# AI Client: Single Client with auto-detection + telemetry integration
# =============================================================================

# =============================================================================
# AI Provider API Keys (Auto-Detection)
# =============================================================================
# The framework automatically detects which provider to use.
# Only ONE key is required.
#
# Detection priority: OpenAI -> Groq -> Anthropic -> Gemini

# OpenAI (RECOMMENDED)
# Get your key: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic (Claude models)
# Get your key: https://console.anthropic.com/
#ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Groq (Ultra-fast inference)
# Get your key: https://console.groq.com/keys
# Free tier: 14,000 tokens/minute
#GROQ_API_KEY=gsk-your-groq-key-here

# Google Gemini
# Get your key: https://aistudio.google.com/apikey
#GEMINI_API_KEY=your-gemini-key-here

# =============================================================================
# Model Alias Overrides (Optional)
# =============================================================================
# Override which model is used without changing code.
# Pattern: GOMIND_{PROVIDER}_MODEL_{ALIAS}=actual-model-name
#
# Aliases: "default", "smart", "fast"

#GOMIND_OPENAI_MODEL_DEFAULT=gpt-4o-mini
#GOMIND_OPENAI_MODEL_SMART=gpt-4o
#GOMIND_ANTHROPIC_MODEL_DEFAULT=claude-3-haiku-20240307
#GOMIND_GROQ_MODEL_DEFAULT=llama-3.1-8b-instant

# =============================================================================
# Service Configuration
# =============================================================================

# Redis URL for service discovery (required)
REDIS_URL=redis://localhost:6379

# HTTP server port
PORT=8092

# Development mode - enables additional debugging
DEV_MODE=true

# =============================================================================
# Telemetry Configuration (Key Feature of This Example)
# =============================================================================
# APP_ENV selects the telemetry profile:
#   - development: Console output, 100% sampling, verbose logging
#   - staging: OTLP export, 10% sampling, balanced
#   - production: OTLP export, 0.1% sampling, minimal overhead

APP_ENV=development

# OpenTelemetry Collector endpoint
# This enables:
#   - Distributed tracing across tool calls (viewable in Jaeger)
#   - Log correlation via trace_id and span_id (for Grafana Loki)
#   - Metrics export to Prometheus
#
# Endpoints:
#   - Local development: http://localhost:4318
#   - Kubernetes: http://otel-collector.gomind-examples:4318
#   - Leave empty for stdout-only telemetry
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# =============================================================================
# Logging Configuration (Optional)
# =============================================================================
# GOMIND_LOG_LEVEL: debug, info, warn, error
GOMIND_LOG_LEVEL=info

# Enable debug logging (alternative to LOG_LEVEL=debug)
#GOMIND_DEBUG=true

# =============================================================================
# Advanced Configuration (Optional)
# =============================================================================

# Enable payload validation with JSON Schema
#GOMIND_VALIDATE_PAYLOADS=false

# =============================================================================
# What to Look For in Jaeger (http://localhost:16686)
# =============================================================================
# After running requests, check Jaeger for:
#   - Service: "research-agent-telemetry"
#   - Spans: ai.generate_response, tool.call, http.request
#   - Attributes: ai.provider, ai.model, ai.prompt_tokens, ai.completion_tokens
#   - Trace propagation across tool services
# =============================================================================
